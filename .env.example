# LLM Configuration
LLM_PROVIDER=openai
LLM_API_KEY=your_openai_api_key_here
LLM_MODEL=gpt-5.2

# OpenAI other models:
# LLM_MODEL=gpt-5-mini  # Smaller, faster variant
# LLM_MODEL=gpt-5-nano  # Smallest, cheapest variant
# LLM_MODEL=gpt-5.1     # Previous version

# Gemini example:
# LLM_PROVIDER=gemini
# LLM_API_KEY=your_gemini_api_key
# LLM_MODEL=gemini-3-flash

# Gemini other models:
# LLM_MODEL=gemini-2.0-flash
# LLM_MODEL=gemini-2.0-flash-lite
# LLM_MODEL=gemini-2.0-pro

# DeepSeek example:
# LLM_PROVIDER=openai
# LLM_API_KEY=your_deepseek_api_key
# LLM_MODEL=deepseek-chat
# LLM_BASE_URL=https://api.deepseek.com/v1

# DeepSeek reasoning model:
# LLM_MODEL=deepseek-reasoner  # For complex reasoning tasks

# xAI Grok example:
# LLM_PROVIDER=openai
# LLM_API_KEY=your_xai_api_key
# LLM_MODEL=grok-4-1-fast-reasoning
# LLM_BASE_URL=https://api.x.ai/v1

# Grok other models:
# LLM_MODEL=grok-4
# LLM_MODEL=grok-3-mini  # Smaller, faster variant

# Ollama example (local):
# LLM_PROVIDER=openai
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.2
# LLM_BASE_URL=http://localhost:11434/v1

# Notification Service (Optional)
PUSHOVER_USER=your_pushover_user_key
PUSHOVER_TOKEN=your_pushover_token

# FastAPI Middleware Configuration (Required for API mode)
API_KEY=your_secret_api_key_here
ALLOWED_ORIGINS=http://localhost:3000,https://your-portfolio.com

# Rate Limiting Configuration
# Enable/disable rate limiting (can be toggled via admin API)
RATE_LIMIT_ENABLED=true
# Number of requests allowed per hour per IP address
RATE_LIMIT_PER_HOUR=10
# Note: Rate limiting can be dynamically toggled via POST /api/v1/admin/rate-limit
